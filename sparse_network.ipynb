{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse import dok_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import math\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sparse_linnear(nn.Module):\n",
    "    def __init__(self, dimensions):\n",
    "        \"\"\"\n",
    "        :param dimensions: (tpl/ list) Dimensions of the neural net. (input, output)\n",
    "        Example of three hidden layer with\n",
    "        - input features\n",
    "        - output features\n",
    "        layers -->    [0,        1]\n",
    "        ----------------------------------------\n",
    "        dimensions =  (input features,     output features)\n",
    "        \"\"\"\n",
    "        self.n_layers = len(dimensions)\n",
    "        self.loss = None\n",
    "        self.learning_rate = None\n",
    "        self.momentum = None\n",
    "        self.weight_decay = None\n",
    "        self.epsilon = 20  # control the sparsity level as discussed in the paper\n",
    "        self.zeta = 0.3  # the fraction of the weights removed\n",
    "        self.droprate = 0  # dropout rate\n",
    "        self.dimensions = dimensions\n",
    "        # activations will be chosen from <activations>\n",
    "        self.activations = ['Relu', 'Sigmoid', 'tanh']\n",
    "        # Weights and biases are initiated by index. For a one hidden layer net you will have a w[1] and w[2]\n",
    "        self.w = [] # weight matrix\n",
    "        self.x = [] # adjacency matrix\n",
    "        self.b = np.zeros(dimensions[1])\n",
    "        # Here is the gradients that need to be backpropagated (I will write them in the future)\n",
    "        \n",
    "    def prob_connection(self, T, R, Theta, m, rj, theta_j, beta):\n",
    "        # calculate the probability connections\n",
    "        if T == 0:\n",
    "            dij = {}\n",
    "            for i in range(len(R)):\n",
    "                d = hyperbolic_dist(R[i], rj, hyperbolic_angle(Theta[i], theta_j))\n",
    "                dij[d] = i\n",
    "\n",
    "            dist = sorted(dij.keys(), reverse = True)\n",
    "            idx = []\n",
    "            for i in range(m):\n",
    "                idx.append(dij[dist[i]])\n",
    "\n",
    "            return idx\n",
    "\n",
    "        else:\n",
    "            idx = []\n",
    "            for i in range(len(R)):\n",
    "                theta_ij = self.hyperbolic_angle(Theta[i], theta_j)\n",
    "                hij = self.hyperbolic_dist(R[i], rj, theta_ij)\n",
    "                radius_i = self.radius_of_Hyperbolic_disk(R[i], T, beta, i+2)\n",
    "                prob_ij = 1/(1 + math.exp((hij - radius_i)/(2*T)))\n",
    "                p = random.random()\n",
    "                if prob_ij >= p:\n",
    "                    idx.append(i)\n",
    "            return idx\n",
    "        \n",
    "    def radius_of_Hyperbolic_disk(self, ri, T, beta, i):\n",
    "        Ri = ri - 2*math.log(2*T*(1-math.exp(-(1-beta)*math.log(i)))\n",
    "                             /(np.sin(T*np.pi) * m * (1-beta)))\n",
    "        return Ri\n",
    "\n",
    "    def hyperbolic_dist(self, ri, rj, theta_ij):\n",
    "        hij = np.arccosh(np.cosh(ri) * np.cosh(rj) - np.sinh(ri) * np.sinh(rj) * np.cos(theta_ij))\n",
    "        return hij\n",
    "\n",
    "    def hyperbolic_angle(self, theta_i, theta_j):\n",
    "        theta_ij = np.pi - abs(np.pi - abs(theta_i-theta_j))\n",
    "        return theta_ij\n",
    "    \n",
    "    def createSparseWeights_with_nPSO_Gaussian(self, N, T, gamma, m, C):\n",
    "        # generate an nPSO sparse weights mask\n",
    "        beta = 1 / (gamma - 1)\n",
    "        sigma = 2 * np.pi/(6 * C)\n",
    "        p = 1/C\n",
    "        R_1_2 = [] # two layers of nerouns\n",
    "        Theta_1_2 = [] # the angle of nerouns reflect in the hyperbolic space\n",
    "        # for bipartite network, two layers of neurons\n",
    "        for k in range(2):\n",
    "            R = []\n",
    "            Theta = []\n",
    "            for i in range(1,N+1):\n",
    "                r = 2 * math.log(i) \n",
    "                # generate angle with gauss distribution\n",
    "                mu = 2 / C  * np.pi * ((i-1) % C)\n",
    "                theta = random.gauss(mu, sigma) %  (2 * np.pi)\n",
    "                R.append(r)\n",
    "                Theta.append(theta)\n",
    "                if i > 1:\n",
    "                    for j in range(i-1):\n",
    "                        R[j] = beta * 2 * math.log(j + 1) + (1-beta)* 2 * math.log(i) \n",
    "            R_1_2.append(R)\n",
    "            Theta_1_2.append(Theta)\n",
    "\n",
    "        ax = plt.subplot(111, projection='polar')\n",
    "        c = ax.scatter(Theta, R, cmap='cool', alpha=1)\n",
    "        row = []\n",
    "        col = []\n",
    "        data1 = []\n",
    "        data2 = []\n",
    "        for i in range(len(R_1_2[0])):\n",
    "            # idx is the connected links calculated by nPSO\n",
    "            idx = self.prob_connection(T, R_1_2[1], Theta_1_2[1], m, R_1_2[0][i], Theta_1_2[0][i], beta)   \n",
    "            for j in idx:\n",
    "                row.append(i)\n",
    "                col.append(j)\n",
    "                # initialize the weight\n",
    "                data1.append(np.float64(np.random.randn() / 10))\n",
    "                data2.append(1)\n",
    "        # initialize the sparse weight matrix\n",
    "#         data1 = nn.Parameter((len(row)))\n",
    "        weight_matrix = coo_matrix((data1, (row,col)),shape=(N,N))\n",
    "        x = coo_matrix((data2, (row,col)), shape=(N,N))\n",
    "        print(\"Create sparse matrix with \", weight_matrix.getnnz(), \" connections and \",\n",
    "                      (weight_matrix.getnnz() / (N * N)) * 100, \"% density level\")\n",
    "        self.w = weight_matrix\n",
    "        self.x = x.toarray()\n",
    "        return weight_matrix.tocsr()\n",
    "    \n",
    "    \n",
    "    def createSparseWeights_with_ER(self, epsilon, noRows, noCols):\n",
    "        # generate an Erdos Renyi sparse weights mask\n",
    "        # epsilon: control the sparsity level as discussed in the paper\n",
    "        # noRows, noCols: the numbers of two layers of nerouns\n",
    "        weights = lil_matrix((noRows, noCols))\n",
    "        for i in range(epsilon * (noRows + noCols)):\n",
    "            weights[np.random.randint(0, noRows), np.random.randint(0, noCols)] = np.float64(np.random.randn() / 10)\n",
    "        print(\"Create sparse matrix with \", weights.getnnz(), \" connections and \",\n",
    "              (weights.getnnz() / (noRows * noCols)) * 100, \"% density level\")\n",
    "        self.w = weights.tocsr()\n",
    "        return weights.tocsr()\n",
    "    \n",
    "    \n",
    "\n",
    "    def weightsEvolution_I(self):\n",
    "#         this represents the core of the SET procedure. \n",
    "#         It removes the weights closest to zero in each layer and add new random weights\n",
    "\n",
    "        values = np.sort(self.w.data)\n",
    "        firstZeroPos = self.find_first_pos(values, 0)\n",
    "        lastZeroPos = self.find_last_pos(values, 0)\n",
    "\n",
    "        largestNegative = values[int((1 - self.zeta) * firstZeroPos)]\n",
    "        smallestPositive = values[\n",
    "            int(min(values.shape[0] - 1, lastZeroPos + self.zeta * (values.shape[0] - lastZeroPos)))]\n",
    "\n",
    "        wlil = self.w.tolil()\n",
    "        wdok = dok_matrix((self.dimensions[0], self.dimensions[1]), dtype=\"float64\")\n",
    "\n",
    "\n",
    "        # remove the weights closest to zero\n",
    "        keepConnections = 0\n",
    "        for ik, (row, data) in enumerate(zip(wlil.rows, wlil.data)):\n",
    "            for jk, val in zip(row, data):\n",
    "                if ((val < largestNegative) or (val > smallestPositive)):\n",
    "                    wdok[ik, jk] = val\n",
    "\n",
    "                    keepConnections += 1\n",
    "\n",
    "        # add new random connections\n",
    "        for kk in range(self.w.data.shape[0] - keepConnections):\n",
    "            ik = np.random.randint(0, self.dimensions[0])\n",
    "            jk = np.random.randint(0, self.dimensions[1])\n",
    "            while (wdok[ik, jk] != 0):\n",
    "                ik = np.random.randint(0, self.dimensions[0])\n",
    "                jk = np.random.randint(0, self.dimensions[1])\n",
    "            wdok[ik, jk] = np.random.randn() / 10\n",
    "            \n",
    "        self.w = wdok.tocsr()\n",
    "        return wdok.tocsr()\n",
    "    \n",
    "    def weightsEvolution_II(self):\n",
    "#       this represents the core of the CH2-L3 procedure. It removes the weights \n",
    "#       closest to zero in each layer and add new weights with CHA methods\n",
    "\n",
    "        values = np.sort(self.w.data)\n",
    "        firstZeroPos = self.find_first_pos(values, 0)\n",
    "        lastZeroPos = self.find_last_pos(values, 0)\n",
    "\n",
    "        largestNegative = values[int((1 - self.zeta) * firstZeroPos)]\n",
    "        smallestPositive = values[\n",
    "            int(min(values.shape[0] - 1, lastZeroPos + self.zeta * (values.shape[0] - lastZeroPos)))]\n",
    "\n",
    "        wlil = self.w.tolil()\n",
    "        wdok = dok_matrix((self.dimensions[0], self.dimensions[1]), dtype=\"float64\")\n",
    "\n",
    "\n",
    "        # remove the weights closest to zero\n",
    "        keepConnections = 0\n",
    "        for ik, (row, data) in enumerate(zip(wlil.rows, wlil.data)):\n",
    "            for jk, val in zip(row, data):\n",
    "                if ((val < largestNegative) or (val > smallestPositive)):\n",
    "                    wdok[ik, jk] = val\n",
    "                    keepConnections += 1\n",
    "        \n",
    "        # add new random connections with CH2_L3\n",
    "        non_connect_nodes, sorted_index = self.sorted_non_connected_links_with_CH2_L3()\n",
    "        weight_values = np.float64(np.random.randn(self.w.data.shape[0] - keepConnections, 1) / 10)\n",
    "        self.w = wdok.tocoo()\n",
    "        row, col = self.w.nonzero()\n",
    "        data1 = self.w.data\n",
    "        for i in range(len(weight_values)):\n",
    "            row = np.append(row,non_connect_nodes[sorted_index[i]][0])\n",
    "            col = np.append(col,non_connect_nodes[sorted_index[i]][1])\n",
    "            data1 = np.append(data1, weight_values[i])\n",
    "            self.x[non_connect_nodes[sorted_index[i]][0]][non_connect_nodes[sorted_index[i]][1]] = 1\n",
    "        self.w = coo_matrix((data1, (row,col)),shape = (self.dimensions[0], self.dimensions[1]))\n",
    "\n",
    "        return self.w\n",
    "        \n",
    "    def sorted_non_connected_links_with_CH2_L3(self):\n",
    "        row_sum = np.reshape(self.x.sum(axis = 1), (100,-1))\n",
    "        col_sum = np.reshape(self.x.sum(axis = 0), (100,-1))\n",
    "\n",
    "        non_connect_nodes = find_zero_items(self.x)\n",
    "        m = len(non_connect_nodes)\n",
    "        A1 = []\n",
    "        A2 = []\n",
    "        for i in range(len(self.x)):\n",
    "            A1.append(np.nonzero(self.x[i,:]))\n",
    "            A2.append(np.nonzero(self.x[:,i]))\n",
    "        CH2_L3_scores = []\n",
    "        for i in range(m):\n",
    "            u = non_connect_nodes[i][0]\n",
    "            v = non_connect_nodes[i][1]\n",
    "            Au = A2[v]\n",
    "            Av = A1[u]\n",
    "            x_Auv = self.x[Au[0]]\n",
    "            x_Auv = x_Auv[:,Av[0]]\n",
    "\n",
    "            a, b = np.nonzero(x_Auv)\n",
    "            dict_a = get_item_nums(a)\n",
    "            dict_b = get_item_nums(b)\n",
    "\n",
    "            scores_CH2_L3 = 0\n",
    "            for j in range(len(a)):\n",
    "                i_num_x = dict_a[a[j]]\n",
    "                i_num_y = dict_b[b[j]]\n",
    "                idx_x = Au[0][a[j]]\n",
    "                idx_y = Av[0][b[j]]\n",
    "                deg_x = row_sum[idx_x]\n",
    "                deg_y = col_sum[idx_y]\n",
    "                e_num_x = deg_x - i_num_x\n",
    "                e_num_y = deg_y - i_num_y\n",
    "                scores_CH2_L3 += math.sqrt((1 + i_num_x)*(1 + i_num_y))/math.sqrt((1 + e_num_x)*(1 + e_num_y))\n",
    "            CH2_L3_scores.append(scores_CH2_L3)\n",
    "        dict_scores = {j:i for i, j in enumerate(CH2_L3_scores)}\n",
    "        sorted_scores = sorted(list(dict_scores.keys()), reverse =True)\n",
    "        index = []\n",
    "        for i in sorted_scores:\n",
    "            index.append(dict_scores[i])\n",
    "        \n",
    "        return non_connect_nodes, index\n",
    "    \n",
    "    def find_first_pos(self, array, value):\n",
    "        idx = (np.abs(array - value)).argmin()\n",
    "        return idx\n",
    "    \n",
    "    def find_last_pos(self, array, value):\n",
    "        idx = (np.abs(array - value))[::-1].argmin()\n",
    "        return array.shape[0] - idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_zero_items(A):\n",
    "    non_connect_nodes = []\n",
    "    for i in range(len(A)):\n",
    "        for j in range(len(A)):\n",
    "            if A[i,j] == 0:\n",
    "                non_connect_nodes.append([i,j])\n",
    "                \n",
    "    return non_connect_nodes\n",
    "\n",
    "def get_item_nums(arr):\n",
    "    key = np.unique(arr)\n",
    "    result = {}\n",
    "    for k in key:\n",
    "        mask = (arr == k)\n",
    "        arr_new = arr[mask]\n",
    "        v = arr_new.size\n",
    "        result[k] = v\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sparse_linnear((100,100))\n",
    "N = 100\n",
    "T = 0.6\n",
    "gamma = 3\n",
    "m = 15\n",
    "C = 4\n",
    "w = model.createSparseWeights_with_nPSO_Gaussian(N, T, gamma, m, C)\n",
    "x = model.x\n",
    "# w = torch.Tensor(w.toarray())\n",
    "# scipy_sparse_mat_to_torch_sparse_tensor(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(w.toarray(), cmap='jet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)\n",
    "w1 = torch.Tensor(w.toarray()).cuda()\n",
    "w1.requires_grad_()\n",
    "x = torch.Tensor(x).cuda()\n",
    "input1 = torch.randn(100, 1).cuda()\n",
    "input2 = torch.randn(100, 1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "criterion = torch.nn.MSELoss()\n",
    "start = datetime.datetime.now()\n",
    "for i in range(100000):\n",
    "    w2 = w1 * x\n",
    "    output = torch.mm(w2, input1)\n",
    "    loss = criterion(output, input2)\n",
    "    optimizer = torch.optim.SGD([w1], lr = 0.1)\n",
    "    optimizer.zero_grad()   # reset gradient\n",
    "    loss.backward()\n",
    "    optimizer.step()  \n",
    "    \n",
    "end = datetime.datetime.now()\n",
    "print(\"Start running time: %s\" % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "start = datetime.datetime.now()\n",
    "for i in range(100000):\n",
    "#     w2 = w1 * x\n",
    "    output = torch.mm(w1, input1)\n",
    "    loss = criterion(output, input2)\n",
    "    optimizer = torch.optim.SGD([w1], lr = 0.1)\n",
    "    optimizer.zero_grad()   # reset gradient\n",
    "    loss.backward()\n",
    "    optimizer.step()  \n",
    "    \n",
    "end = datetime.datetime.now()\n",
    "print(\"Start running time: %s\" % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scipy_sparse_mat_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"\n",
    "    将scipy的sparse matrix转换成torch的sparse tensor.\n",
    "    \"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = scipy_sparse_mat_to_torch_sparse_tensor(w)\n",
    "\n",
    "w_values = w._values().requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_scatter import scatter\n",
    "\n",
    "input1 = torch.randn(100, 1)\n",
    "in_ = torch.empty(len(w_values))\n",
    "input2 = torch.randn(100, 1)\n",
    "output = torch.empty(len(w._indices()[0]))\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([w_values], lr = 0.1)\n",
    "for idx in range(len(w._indices()[0])):\n",
    "    in_[idx] = input1[w._indices()[0][idx]]\n",
    "start = datetime.datetime.now()\n",
    "for i in range(100000):\n",
    "    optimizer.zero_grad()   # reset gradient\n",
    "#     for idx in range(len(w._indices()[0])):\n",
    "#         output[idx] = input1[w._indices()[0][idx]]*w_values[idx]\\\n",
    "    \n",
    "    output = in_ * w_values\n",
    "    src = output\n",
    "    index = w._indices()[1]\n",
    "#     out = torch.zeros(100)\n",
    "#     for idx in range(len(w._indices()[0])):\n",
    "#         out[w._indices()[1][idx]] += src[idx]\n",
    "    out = scatter(src, index, dim=0, reduce=\"sum\")\n",
    "\n",
    "    loss = criterion(out, input2)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "#     print(1)\n",
    "    \n",
    "end = datetime.datetime.now()\n",
    "print(\"Start running time: %s\" % (end-start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
